{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(csv_file, sequence_length):\n",
    "    \n",
    "  \n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    features = df[['magnitude', 'cdi', 'mmi', 'dmin', 'gap', 'depth', 'latitude', 'longitude']].values\n",
    "    \n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    normalized_features = scaler.fit_transform(features)\n",
    "    \n",
    "    sequences = []\n",
    "    for i in range(0, len(normalized_features) - sequence_length + 1, sequence_length // 2):  \n",
    "        sequence = normalized_features[i:i + sequence_length]\n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    return torch.FloatTensor(sequences), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47305/52243508.py:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400268359/work/torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  return torch.FloatTensor(sequences), scaler\n"
     ]
    }
   ],
   "source": [
    "normal_sequences, normal_scaler = preprocess_data(\"Normal.csv\", 50)\n",
    "anomalous_sequences, anomalous_scaler = preprocess_data(\"Anomalous.csv\", 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from TsunoGan import EnhancedAnoGAN, TsunamiDetector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: G_loss: 0.7269, D_loss: 0.7009\n",
      "Epoch 10: G_loss: 0.6994, D_loss: 0.6729\n",
      "Epoch 20: G_loss: 0.8766, D_loss: 0.6027\n",
      "Epoch 30: G_loss: 0.9920, D_loss: 0.4188\n",
      "Epoch 40: G_loss: 1.2661, D_loss: 0.4137\n",
      "Epoch 50: G_loss: 1.5360, D_loss: 0.2694\n",
      "Epoch 60: G_loss: 1.3942, D_loss: 0.3263\n",
      "Epoch 70: G_loss: 2.0144, D_loss: 0.1804\n",
      "Epoch 80: G_loss: 2.0841, D_loss: 0.1615\n",
      "Epoch 90: G_loss: 2.4833, D_loss: 0.1012\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "sequence_length = 50  # Sequence length for training (you already have it)\n",
    "latent_dim = 100  # Latent dimension for the generator (can be adjusted)\n",
    "lambda_ano = 0.1  # Weight for anomaly loss\n",
    "num_features = 8  # Number of features (as in the input data)\n",
    "\n",
    "\n",
    "# Create the TsunamiDetector instance\n",
    "tsunami_detector = TsunamiDetector(sequence_length, latent_dim, lambda_ano, num_features,device='cpu')\n",
    "\n",
    "# Train the model on the normal sequences (normal_sequences is already preprocessed)\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# Convert normal_sequences to a DataLoader compatible format (TensorDataset)\n",
    "normal_data = TensorDataset(normal_sequences)\n",
    "normal_dataloader = DataLoader(normal_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "tsunami_detector.train(normal_dataloader, epochs=epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated threshold: 2.6446691155433655\n"
     ]
    }
   ],
   "source": [
    "# Calibrate threshold with normal data\n",
    "validation_data = normal_sequences  # You can use a validation set or a portion of normal data\n",
    "threshold_percentile = 95  # You can adjust this percentile based on your needs\n",
    "threshold = tsunami_detector.calibrate_threshold(validation_data, percentile=threshold_percentile)\n",
    "\n",
    "print(f\"Calibrated threshold: {threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Tsunami: False, Confidence: 0.0, Anomaly Score: 1.8137266635894775\n",
      "Is Tsunami: False, Confidence: 0.0, Anomaly Score: 2.1949853897094727\n",
      "Is Tsunami: False, Confidence: 0.0, Anomaly Score: 1.949520468711853\n",
      "Is Tsunami: False, Confidence: 0.0, Anomaly Score: 1.6330344676971436\n",
      "Is Tsunami: False, Confidence: 0.0, Anomaly Score: 1.9007203578948975\n",
      "Is Tsunami: False, Confidence: 0.0, Anomaly Score: 1.8562805652618408\n",
      "Is Tsunami: False, Confidence: 0.0, Anomaly Score: 1.8144011497497559\n",
      "Is Tsunami: False, Confidence: 0.0, Anomaly Score: 1.5864073038101196\n",
      "Is Tsunami: False, Confidence: 0.0, Anomaly Score: 1.41500723361969\n",
      "Is Tsunami: False, Confidence: 0.0, Anomaly Score: 1.572488784790039\n",
      "Is Tsunami: False, Confidence: 0.0, Anomaly Score: 1.687887191772461\n",
      "Is Tsunami: False, Confidence: 0.0, Anomaly Score: 1.8075538873672485\n"
     ]
    }
   ],
   "source": [
    "# Assuming anomalous_sequences is a tensor of anomalous data (like actual tsunami events)\n",
    "anomalous_sequences = torch.FloatTensor(anomalous_sequences)  # Ensure it's a tensor if not already\n",
    "\n",
    "# Evaluate anomalous data\n",
    "for sequence in anomalous_sequences:\n",
    "    result = tsunami_detector.detect_tsunami(sequence)\n",
    "    print(f\"Is Tsunami: {result['is_tsunami']}, Confidence: {result['confidence']}, Anomaly Score: {result['anomaly_score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
